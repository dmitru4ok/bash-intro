#!/usr/bin/env bash
PATH=.

if [ -z $1 ]; then
    echo "Usage: $0 text_file [db_noun_file]" >&2
    exit 65
fi

if [ ! -f $1 ]; then
    echo "File $1 doesn't exist..." >&2
    exit 66
fi

if [ ! -r "$1" ]; then
    echo "Permission error: $1." >&2
    exit 66
fi


if [ -z "$2" ]; then
    NOUN_DB_PATH=/opt/task1/resources/noundb.txt
    echo -e "\e[0;33mNo db_noun_file is provided. Using $NOUN_DB_PATH as a default\e[0m" >&2
else
    NOUN_DB_PATH="$2"
fi


if [ ! -f "$NOUN_DB_PATH" ]; then
    echo "File $NOUN_DB_PATH doen't exist" >&2
    exit 66
fi

if [ ! -r "$NOUN_DB_PATH" ]; then
    echo "Permission error: $NOUN_DB_PATH" >&2
    exit 66
fi



qmem_index=0
declare -a PREPROCESSED
while IFS= read -r -n 1 char; do
    if [[ "$char" == "!" || "$char" == "?" ]]; then
        ((qmem_index++))
    elif [ -z "$char" ]; then 
        PREPROCESSED["$qmem_index"]+=' '
    else
        PREPROCESSED["$qmem_index"]+="${char//[,\’\‘\";–:()\`\_\—\@\#\$\%\^\&\*\+\=\{\}\[\]]/\ }"
    fi
done < "$1"


declare -a TOKENIZED
declare -A DISPLAYED_MAP
index=0
previous_preprocessed=""
LEN_TRESHOLD=1
declare -A USUAL_DOTTED=( [p.m.]=1 [a.m.]=1 [etc.]=1 [e.g.]=1 [al.]=1 [fac.]=1 [i.e.]=1 [p.a.]=1 
    [t.t.]=1 [t.p.]=1 [t.y.]=1 [pvz.]=1 [kt.]=1 [pan.]=1 [žr.]=1 [prof.]=1 [dr.]=1 [akad.]=1 [doc.]=1 [phd.]=1 [adj.]=1 [plg.]=1 
    [lit.]=1 [inž.]=1 [mln.]=1 [mlrd.]=1 [met.]=1 [d.]=1 [val.]=1 [min.]=1 [sek.]=1 [a.]=1 [pav.]=1
)

display_sentence_stats() {
    [ "${#TOKENIZED[@]}" -eq 0 ] && return 1

    declare -A positions
    local max_width=0
    local noun_counter=0
    for word_ind in "${!TOKENIZED[@]}"; do
        word="${TOKENIZED["$word_ind"]}"
        positions["$word"]+="$word_ind "
        [ "${#word}" -gt "$max_width" ] && max_width="${#word}"
        [ -n "${noun_map["$word"]}" ] && ((++noun_counter))
    done
    echo "---------------------------------------------------"
    echo "|${TOKENIZED[@]}|"
    echo "NOUNS: $noun_counter"

    words_unique=("${!positions[@]}")
    for ((ind1=0; ind1<${#words_unique[@]}; ++ind1)); do
        for ((ind2=ind1+1; ind2<${#words_unique[@]}; ++ind2)); do
            
            w1="${words_unique[$ind1]}"
            w2="${words_unique[$ind2]}"
            
            IFS=' ' read -a p1 <<< "${positions["$w1"]}"
            IFS=' ' read -a p2 <<< "${positions["$w2"]}"
            sum=0
            counter=0
            for pos1 in "${p1[@]}"; do
                for pos2 in "${p2[@]}"; do
                    exp=$(( $pos1 - $pos2 > 0 ? $pos1 - $pos2 : $pos2 - $pos1 ))
                    if [ "$exp" -ne 0 ]; then
                        ((++counter))
                        ((sum+=$exp))
                    fi
                done
            done
            if [ "$counter" -ne 0 ]; then
                result=$(($sum * 100 / $counter))
                printf "|%-*s| <-> |%-*s|: AVG dist = %d.%02d\n" "$max_width" "$w1" "$max_width" "$w2" "$(($result / 100))" "$(($result % 100))"
            fi
        done
    done
    echo "---------------------------------------------------"
    TOKENIZED=()
}

declare -A noun_map
while read -r line; do
    noun_map["$line"]=1
done < "$NOUN_DB_PATH"

declare -l lower
pp_index=0
while [ "$pp_index" -le "$qmem_index" ]; do 
    IFS=' ' read -a words <<< "${PREPROCESSED["$pp_index"]}"
    for current in "${words[@]}"; do
        current_preprocessed="${current#\'}"
        current_preprocessed="${current_preprocessed%\'}"
        echo "$current"
        if [ -n "$previous_preprocessed" ]; then
            if [[ "${current_preprocessed}" =~ ^[A-Z] && "${#current_preprocessed}" -gt "$LEN_TRESHOLD" ]]; then
                TOKENIZED+=("${previous_preprocessed,,}")
                display_sentence_stats
                TOKENIZED+=("${current_preprocessed,,}")
            else
                TOKENIZED+=("${previous_preprocessed,,}")
                TOKENIZED+=("${current_preprocessed,,}")
            fi
            previous_preprocessed=""
        elif [[ "$current_preprocessed" =~ ^[0-9]*\.$ ]]; then
            dotless="${current_preprocessed//.}"
            TOKENIZED+=("${dotless,,}")
            display_sentence_stats
        elif [[ "$current_preprocessed" =~ ^[A-Z].*\.$ ]]; then
            TOKENIZED+=("${current_preprocessed,,}")
        elif [[ "$current_preprocessed" =~ [\.]$ ]]; then
            lower="${current_preprocessed}"
            if [ -n "${USUAL_DOTTED[$lower]}" ]; then
                TOKENIZED+=("${lower//.}")
                continue
            fi
            previous_preprocessed="${current_preprocessed//.}"
        else
            TOKENIZED+=("${current_preprocessed,,}")
        fi
    done
    if [ -n "$previous_preprocessed" ]; then
        TOKENIZED+=("${previous_preprocessed,,}")
        previous_preprocessed=""
    fi
    display_sentence_stats
    ((pp_index++))
done
